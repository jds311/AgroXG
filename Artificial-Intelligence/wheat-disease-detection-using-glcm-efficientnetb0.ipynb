{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wheat Disease Detection on Leaf Images using EfficientNetB0","metadata":{}},{"cell_type":"markdown","source":"## Import Data from Google Drive Folder","metadata":{}},{"cell_type":"code","source":"!conda install -y gdown \nimport gdown ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = 'https://drive.google.com/uc?id=1rdbF95HfP4K_lznMirByqvkYzEzaavg-'\noutput = 'input.zip'\ngdown.download(url, output)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T22:19:28.259435Z","iopub.execute_input":"2021-10-21T22:19:28.259701Z","iopub.status.idle":"2021-10-21T22:20:14.265405Z","shell.execute_reply.started":"2021-10-21T22:19:28.259666Z","shell.execute_reply":"2021-10-21T22:20:14.264663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip input.zip","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install imutils","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:58:38.716027Z","iopub.execute_input":"2021-11-24T12:58:38.716371Z","iopub.status.idle":"2021-11-24T12:58:48.966857Z","shell.execute_reply.started":"2021-11-24T12:58:38.716288Z","shell.execute_reply":"2021-11-24T12:58:48.966031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm input.zip","metadata":{"execution":{"iopub.status.busy":"2021-10-21T22:20:40.972364Z","iopub.execute_input":"2021-10-21T22:20:40.972924Z","iopub.status.idle":"2021-10-21T22:20:42.098889Z","shell.execute_reply.started":"2021-10-21T22:20:40.972884Z","shell.execute_reply":"2021-10-21T22:20:42.097956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Libraries and Imports","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import AveragePooling2D, Dense, Dropout, Flatten, Input\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom imutils import paths\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:58:48.969116Z","iopub.execute_input":"2021-11-24T12:58:48.969401Z","iopub.status.idle":"2021-11-24T12:58:54.34073Z","shell.execute_reply.started":"2021-11-24T12:58:48.969366Z","shell.execute_reply":"2021-11-24T12:58:54.339894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Extract GLCM and GLCM Features","metadata":{}},{"cell_type":"code","source":"#! /usr/bin/python\n# - *- coding: UTF-8-*-\nimport cv2\nimport math\nfrom numpy.linalg import matrix_power\n\n# Define the maximum number of gray levels\ngray_level =128\n\ndef maxGrayLevel(img):\n    max_gray_level=0\n    (height,width)=img.shape\n    #print(\"The height and width of the image are: height,width\",height,width)\n    for y in range(height):\n        for x in range(width):\n            if img[y][x]> max_gray_level:\n                max_gray_level = img[y][x]\n                #print(\"max_gray_level:\",max_gray_level)\n    return max_gray_level+1\n\ndef getGlcm(input,d_x,d_y):\n    srcdata=input.copy()\n    ret=[[0.0 for i in range(gray_level)] for j in range(gray_level)]\n    (height,width)= input.shape\n\n    max_gray_level=maxGrayLevel(input)\n    # If the number of gray levels is greater than gray_level, the gray level of the image is reduced to gray_level, reduce the size of the gray-level co-occurrence matrix\n    if max_gray_level > gray_level:\n        for j in range(height):\n            for i in range(width):\n                srcdata[j][i]= srcdata[j][i]*gray_level / max_gray_level\n\n    for j in range(height-d_y):\n        for i in range(width-d_x):\n            rows = srcdata[j][i]\n            cols = srcdata[j + d_y][i+d_x]\n            ret[rows][cols]+=1.0\n    \n    for i in range(gray_level):\n        for j in range(gray_level):\n            ret[i][j]/=float(height*width)\n    return ret\n\ndef feature_computer(p):\n    # con: Contrast \n    # reflects the sharpness of the image and the depth of the grooves of the texture. \n    # The sharper the texture, the greater the contrast, the greater the contrast.\n    \n    # ent: Entropy \n    # measures the randomness of the amount of information contained in the image \n    # and expresses the complexity of the image. When all the values in the co-occurrence matrix \n    # are equal or the pixel values show the greatest randomness, the entropy is the largest.\n    \n    # asm: (Energy) Angle second-order moment\n    # a measure of the uniformity of image gray distribution and texture thickness. \n    # When the image texture is uniform and regular, the energy value is large; \n    # on the contrary, the element values of the gray-level co-occurrence matrix are similar, and the energy value is small.\n    \n    # idm: (Homogenity) The inverse difference matrix \n    # is also called the inverse variance, which reflects the clarity and regularity of the texture. \n    # The texture is clear, regular, easy to describe, and has a larger value.\n    \n    # cor: Correlation Feature\n    # \n    # \n    \n    # shd: Shade Feature\n    #\n    #\n    \n    # prm: Prominence\n    # \n    #\n    \n    Con=0.0\n    Enp=0.0\n    Asm=0.0\n    Idm=0.0\n    Cor=0.0\n    Shd=0.0\n    Cor=0.0\n    Prm=0.0\n    meean=0.0\n    \n    \n#     for i in range(gray_level):\n#         for j in range(gray_level):\n#             meean=\n#     Cor=corrcoef(p)\n#     A=\n#     Shd=\n    \n    \n    for i in range(gray_level):\n        for j in range(gray_level):\n            Con+=(i-j)*(i-j)*p[i][j]\n            Asm+=p[i][j]*p[i][j]\n            Idm+=p[i][j]/(1+(i-j)*(i-j))\n            \n            if p[i][j]>0.0:\n                Enp+=p[i][j]*math.log(p[i][j])\n    return Asm,Con,-Enp,Idm\n\ndef test_glcm(image_name):\n    img = cv2.imread(image_name)\n    try:\n        img_shape=img.shape\n    except:\n        print('imread error')\n        return\n\n    # If you use &#39;/&#39;Will report TypeError: integer argument expected, got float\n    # In fact, the main error is because of cv2.The parameter in resize is required to be an integer\n    img=cv2.resize(img,(img_shape[1]//2,img_shape[0]//2),interpolation=cv2.INTER_CUBIC)\n\n    img_gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    glcm_0=getGlcm(img_gray,1,0)\n    # glcm_1=getGlcm(src_gray,0,1)\n    # glcm_2=getGlcm(src_gray,1,1)\n    # glcm_3=getGlcm(src_gray,-1,1)print(glcm_0)\n\n    #asm,con,eng,idm=feature_computer(glcm_0)\n    #return [asm,con,eng,idm]\n    return glcm_0","metadata":{"execution":{"iopub.status.busy":"2021-11-24T12:58:54.342188Z","iopub.execute_input":"2021-11-24T12:58:54.342486Z","iopub.status.idle":"2021-11-24T12:58:54.510433Z","shell.execute_reply.started":"2021-11-24T12:58:54.34245Z","shell.execute_reply":"2021-11-24T12:58:54.508841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LABELS = set([\"Crown and Root Rot\", \"Healthy Wheat\", \"Leaf Rust\", \"Wheat Loose Smut\"])\nimagePaths = list(paths.list_images('../input/wheatdiseasedetectionlwdcd/Large Wheat Disease Classification Dataset'))\ndata = []\nlabels = []\n# loop over the image paths\nfor imagePath in imagePaths:\n    # extract the class label from the filename\n    label = imagePath.split(os.path.sep)[-2]\n    print(imagePath, \" -->\", label)\n    # if the label of the current image is not part of the labels\n    # are interested in, then ignore the image\n    if label not in LABELS:\n        continue\n    # load the image, convert it to RGB channel ordering, and resize\n    # it to be a fixed 224x224 pixels, ignoring aspect ratio\n \n    #     image = cv2.imread(imagePath)\n    #     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    #     image = cv2.resize(image, (224, 224))\n    # update the data and labels lists, respectively\n    # [asm,con,eng,idm]=test_glcm(imagePath)\n    glcm=test_glcm(imagePath)\n    data.append(glcm)\n    labels.append(label)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:23:39.646605Z","iopub.execute_input":"2021-11-24T08:23:39.646922Z","iopub.status.idle":"2021-11-24T08:23:39.723333Z","shell.execute_reply.started":"2021-11-24T08:23:39.646836Z","shell.execute_reply":"2021-11-24T08:23:39.722056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GLCM Features Extracted\n\n# import pandas as pd\n# df1=pd.DataFrame(data, columns=[\"Contrast\", \"Entropy\", \"Energy\", \"Homogenity\"])\n# df2=pd.DataFrame(labels, columns=[\"Classes\"])\n# df = pd.merge(df1, df2, right_index=True, left_index=True)\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:48:21.45951Z","iopub.execute_input":"2021-11-23T22:48:21.459959Z","iopub.status.idle":"2021-11-23T22:48:21.471872Z","shell.execute_reply.started":"2021-11-23T22:48:21.459921Z","shell.execute_reply":"2021-11-23T22:48:21.471178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:00:23.44103Z","iopub.execute_input":"2021-11-24T03:00:23.441637Z","iopub.status.idle":"2021-11-24T03:00:23.447113Z","shell.execute_reply.started":"2021-11-24T03:00:23.441595Z","shell.execute_reply":"2021-11-24T03:00:23.446413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[1].shape","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:00:36.061361Z","iopub.execute_input":"2021-11-24T03:00:36.061662Z","iopub.status.idle":"2021-11-24T03:00:36.067394Z","shell.execute_reply.started":"2021-11-24T03:00:36.061631Z","shell.execute_reply":"2021-11-24T03:00:36.066508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0, len(df['Classes'])):\n    if df['Classes'].iloc[i]=='Crown and Root Rot':\n        df['Classes'][i]=0\n    elif df['Classes'].iloc[i]=='Healthy Wheat':\n        df['Classes'][i]=1\n    elif df['Classes'].iloc[i]=='Leaf Rust':\n        df['Classes'][i]=2\n    else :\n        df['Classes'][i]=3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values('Classes')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:56:15.51735Z","iopub.execute_input":"2021-11-23T22:56:15.51806Z","iopub.status.idle":"2021-11-23T22:56:15.539947Z","shell.execute_reply.started":"2021-11-23T22:56:15.518023Z","shell.execute_reply":"2021-11-23T22:56:15.539178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('Features.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T22:58:14.796614Z","iopub.execute_input":"2021-11-23T22:58:14.796864Z","iopub.status.idle":"2021-11-23T22:58:14.846526Z","shell.execute_reply.started":"2021-11-23T22:58:14.796835Z","shell.execute_reply":"2021-11-23T22:58:14.845856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Labelling and Splitting the Data","metadata":{}},{"cell_type":"code","source":"# convert the data and labels to NumPy arrays\ndata = np.array(data)\nlabels = np.array(labels)\n# perform one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\n# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\n(trainX, testX, trainY, testY) = train_test_split(data, labels,\n test_size=0.25, stratify=labels, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T02:43:10.102458Z","iopub.execute_input":"2021-11-24T02:43:10.103012Z","iopub.status.idle":"2021-11-24T02:43:10.400477Z","shell.execute_reply.started":"2021-11-24T02:43:10.102973Z","shell.execute_reply":"2021-11-24T02:43:10.399636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"# initialize the training data augmentation object\ntrainAug = ImageDataGenerator(\n rotation_range=30,\n zoom_range=0.15,\n width_shift_range=0.2,\n height_shift_range=0.2,\n shear_range=0.15,\n horizontal_flip=True,\n fill_mode=\"nearest\")\n# initialize the validation/testing data augmentation object (which\n# we'll be adding mean subtraction to)\nvalAug = ImageDataGenerator()\n# define the ImageNet mean subtraction (in RGB order) and set the\n# the mean subtraction value for each of the data augmentation\n# objects\nmean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\ntrainAug.mean = mean\nvalAug.mean = mean\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T02:43:16.53316Z","iopub.execute_input":"2021-11-24T02:43:16.533613Z","iopub.status.idle":"2021-11-24T02:43:16.539385Z","shell.execute_reply.started":"2021-11-24T02:43:16.533577Z","shell.execute_reply":"2021-11-24T02:43:16.538528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Definition","metadata":{}},{"cell_type":"code","source":"# load the EfficientNet-B0, ensuring the head FC layer sets are left ff\n\nheadmodel = EfficientNetB0(\n    include_top=False,\n    weights=\"imagenet\",\n    input_tensor=Input(shape=(128,128,3)),\n    classes=1000,\n    classifier_activation=\"relu\",\n)\n# construct the head of the model that will be placed on top of the\n# the base model\nmodel = headmodel.output\nmodel = AveragePooling2D(pool_size=(5, 5))(model)\nmodel = Flatten(name=\"flatten\")(model)\nmodel = Dense(512, activation=\"relu\")(model)\nmodel = Dropout(0.4)(model)\nmodel = Dense(len(lb.classes_), activation=\"softmax\")(model)\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmoodel = Model(inputs=headmodel.input, outputs=model)\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the training process\nfor layer in headmodel.layers:\n    layer.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(moodel, to_file='model_EffNetB0.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(moodel, to_file='model_Extended.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model via Transfer Learning","metadata":{}},{"cell_type":"code","source":"# compile our model (this needs to be done after our setting our\n# layers to being non-trainable)\nopt = Adam(learning_rate=1e-3)\nmoodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n               metrics=[\"accuracy\"])\n# train the head of the network for a few epochs (all other layers\n# are frozen) -- this will allow the new FC layers to start to become\n# initialized with actual \"learned\" values versus pure random\nH = moodel.fit(\n    trainAug.flow(trainX, trainY, batch_size=64),\n    steps_per_epoch=len(trainX) // 64,\n    validation_data=valAug.flow(testX, testY),\n    validation_steps=len(testX) // 64,\n    epochs=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extending the Epoches","metadata":{}},{"cell_type":"code","source":"H1 = moodel.fit(\n    trainAug.flow(trainX, trainY, batch_size=64),\n    steps_per_epoch=len(trainX) // 64,\n    validation_data=valAug.flow(testX, testY),\n    validation_steps=len(testX) // 64,\n    epochs=10)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T02:35:17.709462Z","iopub.status.idle":"2021-11-24T02:35:17.710116Z","shell.execute_reply.started":"2021-11-24T02:35:17.709883Z","shell.execute_reply":"2021-11-24T02:35:17.709908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the network\npredictions = moodel.predict(testX, batch_size=64)\nprint(classification_report(testY.argmax(axis=1),\n                            predictions.argmax(axis=1), target_names=lb.classes_))\n# plot the training loss and accuracy\nN = 20\nplt.plot(np.arange(0, N), H.history['accuracy'], label=\"Training Accuracy\")\nplt.plot(np.arange(0, N), H.history['val_accuracy'], label=\"Test Accuracy\")\nplt.title('EfficientNetB0 Model Train vs Test Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()\nplt.savefig(\".\\Accuracy_Plot_EffNetB0.png\")\nplt.plot(H.history['loss'], label=\"Training Loss\")\nplt.plot(H.history['val_loss'], label=\"Test Loss\")\nplt.title('EfficientNetB0 Model Train vs Test Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()\nplt.savefig(\".\\Loss_Plot_EffNetB0.png\")","metadata":{"execution":{"iopub.status.busy":"2021-11-24T02:35:17.711361Z","iopub.status.idle":"2021-11-24T02:35:17.712015Z","shell.execute_reply.started":"2021-11-24T02:35:17.711781Z","shell.execute_reply":"2021-11-24T02:35:17.711805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 10\nplt.plot(np.arange(0, N), H1.history['accuracy'], label=\"Training Accuracy\")\nplt.plot(np.arange(0, N), H1.history['val_accuracy'], label=\"Test Accuracy\")\nplt.title('EfficientNetB0 Model Train vs Test Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()\nplt.savefig(\".\\Accuracy_Plot_EffNetB0.png\")\nplt.plot(H1.history['loss'], label=\"Training Loss\")\nplt.plot(H1.history['val_loss'], label=\"Test Loss\")\nplt.title('EfficientNetB0 Model Train vs Test Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()\nplt.savefig(\".\\Loss_Plot_EffNetB0.png\")","metadata":{"execution":{"iopub.status.busy":"2021-11-24T02:35:17.713231Z","iopub.status.idle":"2021-11-24T02:35:17.7139Z","shell.execute_reply.started":"2021-11-24T02:35:17.713668Z","shell.execute_reply":"2021-11-24T02:35:17.713692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From both the above images, it seems 26 epoch is the best suitable choice.. after 26, it starts overfitting","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}